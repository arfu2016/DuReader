squad 1.1 training dataset: 87599 questions
squad 1.1 validation dataset: 10570 questions

algorithm: BIDAF 简化模型
epochs: 1
batch size: 32
No pretrained word embeddings
python squad2/run_python3.py --train --algo BIDAF --epochs 1 --batch_size 32
There are 30,176,403 parameters in the model

Result:
time: 0.5小时(32线程服务器的cpu，大概用掉20个线程；tensorflow的cpu版本可用多线程和多进程?)
'Bleu-1': 0.089
'Bleu-4': 0.045
'Rouge-L': 0.1585

algorithm: MLSTM 简化模型
epochs: 1
batch size: 32
No pretrained word embeddings
python squad2/run_python3.py --train --algo MLSTM --epochs 1 --batch_size 32
There are 32,023,505 parameters in the model

Result:
time: 1.5小时(32线程服务器的cpu，大概用掉20个线程；tensorflow的cpu版本可用多线程和多进程?)
'Bleu-1': 0.0887
'Bleu-4': 0.0554
'Rouge-L': 0.2736

algorithm: BIDAF 完整模型
epochs: 1
batch size: 32
No pretrained word embeddings
python squad_mlstm/run_python3.py --train --algo BIDAF --epochs 1 --batch_size 32
There are 32,880,003 parameters in the model

Result:
time: 1.5小时(32线程服务器的cpu，大概用掉20个线程；tensorflow的cpu版本可用多线程和多进程?)
'Bleu-1': 0.1347
'Bleu-4': 0.0823
'Rouge-L': 0.338

algorithm: MLSTM 完整模型
epochs: 1
batch size: 32
No pretrained word embeddings
python squad_mlstm/run_python3.py --train --algo MLSTM --epochs 1 --batch_size 32
There are 33,647,105 parameters in the model

Result:
time: 2.5小时(32线程服务器的cpu，大概用掉20个线程)
'Bleu-1': 0.149
'Bleu-4': 0.0885
'Rouge-L': 0.345

algorithm: decanlp
epochs: 8000 (iterations: 8000)
batch size: 128 (查代码后发现，train所用的batch size与validation相同); validation batch size: 128
With pretrained word embeddings
python decaNLP/train.py --train_tasks squad --gpus 0 --train_batch_tokens 5000 --val_batch_size 128
There are 14,469,902 parameters in the model

Result:
time: 1小时（1个gpu，GeForce GTX 1080）
'Bleu-1': 0.525
'Bleu-4': 0.329
'Rouge-L': 0.493

time: 3.5小时（1个gpu，GeForce GTX 1080）
'Bleu-1': 0.643
'Bleu-4': 0.500
'Rouge-L': 0.636

Subset of Baidu Machine Reading Dataset (中文):
trianing: 82909 questions
validation: 5000 questions

algorithm: BIDAF 完整模型
epochs: 1
batch size: 32
No pretrained word embeddings
python run.py --train --algo BIDAF --epochs 1 --batch_size 32 \
--train_files '../data/preprocessed/trainset/search.train2.json' \
--dev_files '../data/preprocessed/devset/search.dev2.json'

There are 75,611,403 parameters in the model

Result:
time: 9小时(32线程服务器的cpu，大概用掉20个线程)
'Bleu-1': 0.361
'Bleu-4': 0.2236
'Rouge-L': 0.2923

Result:
time: 18小时(32线程服务器的cpu，大概用掉20个线程)
'Bleu-1': 0.381
'Bleu-4': 0.2393
'Rouge-L': 0.3025

python work3/ask_and_answer_messi.py --predict --algo BIDAF --batch_size 32
给出一组context，从中选出最适合回答某个问题的context中的一段内容
给出一组context，在其中掺入另外的一句回答，给定某个问题，选出这一组context当中比新掺入
的这句更合适的回答，回答是context当中的一部分内容

搜索：
以往的搜索都是建立在相似逻辑上的，现在的自动问答体现了相似之外的其他的推理逻辑

把自动问答用在搜索上有两个方案：

方案一：

首先用传统词匹配的方法定位与问题相似的文章，或者用分类器的办法把问题进行分类，每个类别是一篇或者
几篇文章。

然后用baidu reader或者decanlp的算法，给定问题，从文章的每一段中都摘出一句或者几句话，摘出的
话都带着自己的概率，文章有好几段话，对摘出的概率进行比较，概率最大的就是最终的结果。

方案二：

首先用传统词匹配的方法定位与问题相似的文章，或者用分类器的办法把问题进行分类，每个类别是一篇或者
几篇文章。

然后用google talk to books的算法，首先把问题转换成句向量，然后把文章转换成一棵词向量、句向量、
段向量组成的树，通过问句向量首先找词向量，看有没有合适的可以作为答案的，如果没有，再找句，再找
段，最后找文章。
